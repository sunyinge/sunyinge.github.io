
<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Text classification | Study notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1. Text Preprocessing How to find the boundaries of words?  Tokenization  Spilt an input sequence into so-called tokens Tokenizer: whitespace, punctuation, treebank (based on a set of rules or heurist">
<meta property="og:type" content="article">
<meta property="og:title" content="Text classification">
<meta property="og:url" content="http://sunyinge.github.io/2020/06/08/Text-classification/index.html">
<meta property="og:site_name" content="Study notes">
<meta property="og:description" content="1. Text Preprocessing How to find the boundaries of words?  Tokenization  Spilt an input sequence into so-called tokens Tokenizer: whitespace, punctuation, treebank (based on a set of rules or heurist">
<meta property="og:image" content="http://sunyinge.github.io/images/whitespace.png">
<meta property="og:image" content="http://sunyinge.github.io/images/punct.png">
<meta property="og:image" content="http://sunyinge.github.io/images/treebank.png">
<meta property="og:image" content="http://sunyinge.github.io/images/tfidf.png">
<meta property="og:image" content="http://sunyinge.github.io/images/1Dconvolution.png">
<meta property="og:image" content="http://sunyinge.github.io/images/pooling.png">
<meta property="og:image" content="http://sunyinge.github.io/images/1Dconchar.png">
<meta property="article:published_time" content="2020-06-09T00:32:21.000Z">
<meta property="article:modified_time" content="2020-06-09T02:12:09.260Z">
<meta property="article:author" content="Yinge Sun">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://sunyinge.github.io/images/whitespace.png">
  
    <link rel="alternative" href="/atom.xml" title="Study notes" type="application/atom+xml">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        
          <a id="nav-github" class="nav-icon" href="https://github.com/sunyinge" target="_blank" rel="noopener"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Study notes</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Hello!</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
      <a class="main-nav-link st-search-show-outputs">Search</a>
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-Text-classification" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/06/08/Text-classification/" class="article-date">
  <time datetime="2020-06-09T00:32:21.000Z" itemprop="datePublished">2020-06-08</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Text classification
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h3 id="text-preprocessing">1. Text Preprocessing</h3>
<p>How to find the boundaries of words?</p>
<ul>
<li><p>Tokenization</p>
<ul>
<li>Spilt an input sequence into so-called tokens</li>
<li>Tokenizer: whitespace, punctuation, treebank (based on a set of rules or heuristics, close to perfect tokenization)</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="/images/whitespace.png" /></th>
<th style="text-align: center;"><img src="/images/punct.png" /></th>
<th style="text-align: center;"><img src="/images/treebank.png" /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><em>Whitespace</em></td>
<td style="text-align: center;"><em>Punctuation</em></td>
<td style="text-align: center;"><em>Treebank</em></td>
</tr>
</tbody>
</table></li>
<li><p>Token normalization</p>
<ul>
<li>Stemming
<ul>
<li>Remove and replace suffixes to get to the root form of the word, which is called <strong>stem</strong></li>
<li>Example: Porter's stemmer
<ul>
<li>Problem: fails on irregular words (e.g., feet <span class="math inline">\(\rightarrow\)</span> feet), produces non-words (e.g., wolves <span class="math inline">\(\rightarrow\)</span> wolv )</li>
</ul></li>
</ul></li>
<li>Lemmatization -- dictionary form
<ul>
<li>Use a vocabulary and morphological analysis to get the base or dictionary form of the word, which is called <strong>lemma</strong></li>
<li>Example: WordNet lemmatizer
<ul>
<li>Problem: not all forms are reduced (e.g., talked <span class="math inline">\(\rightarrow\)</span> talked)</li>
</ul></li>
</ul></li>
</ul>
<p><strong>We need to try stemming or lemmatization and choose best for our task.</strong></p>
<ul>
<li>Further normalization
<ul>
<li>Normalizing capital letters</li>
<li>Acronyms</li>
</ul></li>
</ul></li>
</ul>
<h3 id="feature-extraction-from-text">2. Feature Extraction from Text</h3>
<a id="more"></a>
<h4 id="bag-of-words-bow">1. Bag of Words (BOW)</h4>
<ul>
<li>Count the occurences of a particular token.</li>
<li>For each token we have a feature column, this is called <strong>text vectorization</strong>.</li>
<li>Problems
<ul>
<li><strong>Loose word order</strong>.</li>
<li>Counters are not normalized.</li>
</ul></li>
</ul>
<h4 id="n-grams">2. N-grams</h4>
<ul>
<li>Count token pairs, triplets, etc. E.g., 2-grams for token pairs.</li>
<li><strong>Preserve some ordering</strong>.</li>
<li>Problems
<ul>
<li>Too many features.</li>
</ul></li>
</ul>
<h4 id="tf-idf">3. TF-IDF</h4>
<p>Idea: <strong>remove</strong> some n-grams from features based on their <strong>occurence frequence</strong> in documents of our corpus.</p>
<ul>
<li>High-frequency n-grams:
<ul>
<li>E.g., and, a, the.</li>
<li><strong>Stop-words</strong>, they won't help us discriminate texts.</li>
</ul></li>
<li>Low-frequency n-grams:
<ul>
<li>Typos, rare words.</li>
<li>Likely overfit.</li>
</ul></li>
<li><strong>Medium</strong>-frequency n-grams:
<ul>
<li>Good.</li>
</ul></li>
</ul>
<p><strong>Term frequency (TF)</strong>:</p>
<ul>
<li>Frequency of term (or n-gram) t in document d: <span class="math inline">\(\text{tf}(t,d)\)</span>.</li>
</ul>
<p><strong>Inverse document frequency (IDF)</strong>:</p>
<ul>
<li><span class="math inline">\(N=|D|\)</span>: total number of documents in corpus.</li>
<li><span class="math inline">\(|\{d\in D: t\in d\}|\)</span>: number of documents where term t appears.</li>
<li><span class="math inline">\(\text{idf}(t,D)=log\frac{N}{|\{d\in D: t\in d\}|}\)</span>.</li>
</ul>
<p><strong>TF-IDF</strong>:</p>
<ul>
<li><span class="math inline">\(\text{tfidf}(t,d,D)=\text{tf}(t,d)\cdot\text{idf}(t,D)\)</span>.</li>
<li>A <strong>high TF-IDF</strong> is reached by a high term frequency in the given document and a low document frequency of the term in the whole collection of documents.</li>
</ul>
<p><strong>Better BOW</strong>:</p>
<ul>
<li>Replace counters with TF-IDF.</li>
<li>Normalize the result.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">texts=[</span><br><span class="line">  <span class="string">"good movie"</span>, <span class="string">"not a good movie"</span>, <span class="string">"did not like"</span>, <span class="string">"i like it"</span>, <span class="string">"good one"</span></span><br><span class="line">]</span><br><span class="line">tfidf = TfidfVectorizer(min_df=<span class="number">2</span>, max_df=<span class="number">5</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">features = tfidf.fit_transform(texts)</span><br><span class="line"><span class="comment"># show features in the console</span></span><br><span class="line">pd.DataFrame(</span><br><span class="line">  features.todense(),</span><br><span class="line">  columns=tfidf.get_feature_names()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><img src="/images/tfidf.png" /></p>
<h3 id="deep-learning-for-text-classification">4. Deep Learning for Text Classification</h3>
<ul>
<li>BOW way -- sparse</li>
<li>Neural way -- dense
<ul>
<li><strong>word2vec</strong>: words that have similar conetxt tend to have collinear vectors (smaller cosine distance).</li>
<li>Sum of word2vec vectors can be a good text descriptor already.</li>
</ul></li>
<li><p>A better way: <strong>1D convolutions</strong> (neural networks over embeddings)</p>
<ul>
<li><p>Word embeddings (pretrained vectors)</p></li>
<li><p><strong>Convolution filters</strong></p>
<p><img src="/images/1Dconvolution.png" /></p>
<ul>
<li><p>A filter with kernel width 2 (# row of filter in the graph above) provides high activation for <strong>2-grams with certain meaning</strong>.</p></li>
<li><p>Can be easily extended to 3-grams, 4-grams, 5-grams, ... .</p></li>
<li><p>1D because we slide the window in one direction.</p></li>
<li><p>One filter produces one feature (after pooling), 300 filters produce 300 features and then we can apply MLP on it.</p></li>
<li><p>Padding: maintain same size after concolution filtering by adding leading zeros.</p></li>
<li><p><strong>Pooling</strong>: keep the largest (maximum pooling) or average value (average pooling).</p>
<p><img src="/images/pooling.png" /></p></li>
</ul>
<p>Convolution networks on top of <strong>characters</strong> are called learning from scratch, which works best for large datasets where it beats classical approaches (like BOW) and even beats LSTM that works on word level.</p>
<p><img src="/images/1Dconchar.png" /></p></li>
</ul></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>

        <a data-url="http://sunyinge.github.io/2020/06/08/Text-classification/" data-id="ckbylrrxf000dv83xb7orfhn1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/06/09/End-symbol-in-n-gram-models/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          What&#39;s the real need for an end-symbol in n-gram models?
        
      </div>
    </a>
  
  
    <a href="/2020/06/08/Matrix-derivatives/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Matrix derivatives</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/06/24/Python/">Python</a>
          </li>
        
          <li>
            <a href="/2020/06/23/Probabilistic-graphic-models-for-sequence-tagging/">Probabilistic graphic models for sequence tagging</a>
          </li>
        
          <li>
            <a href="/2020/06/09/End-symbol-in-n-gram-models/">What&#39;s the real need for an end-symbol in n-gram models?</a>
          </li>
        
          <li>
            <a href="/2020/06/08/Text-classification/">Text classification</a>
          </li>
        
          <li>
            <a href="/2020/06/08/Matrix-derivatives/">Matrix derivatives</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/" rel="tag">DL</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Installation/" rel="tag">Installation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/" rel="tag">ML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization/" rel="tag">Optimization</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Yinge Sun<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="#search" class="mobile-nav-link st-search-show-outputs">Search</a>
</nav>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    menuSettings: {
      zoom: "None"
    },
    showMathMenu: false,
    jax: ["input/TeX","output/CommonHTML"],
    extensions: ["tex2jax.js"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
  });
</script>

<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script>



<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
