
<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Probabilistic graphic models for sequence tagging | Study notes</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Sequence tagging, such as part of speech (POS) tagging and named entity recognition (NER). Let \(\mathbf{x}&#x3D;x_1,x_2,\ldots,x_T\) be a sequence of words and \(\mathbf{y}&#x3D;y_1,y_2,\ldots,y_T\) be a seque">
<meta property="og:type" content="article">
<meta property="og:title" content="Probabilistic graphic models for sequence tagging">
<meta property="og:url" content="http://sunyinge.github.io/2020/06/23/Probabilistic-graphic-models-for-sequence-tagging/index.html">
<meta property="og:site_name" content="Study notes">
<meta property="og:description" content="Sequence tagging, such as part of speech (POS) tagging and named entity recognition (NER). Let \(\mathbf{x}&#x3D;x_1,x_2,\ldots,x_T\) be a sequence of words and \(\mathbf{y}&#x3D;y_1,y_2,\ldots,y_T\) be a seque">
<meta property="og:image" content="http://sunyinge.github.io/images/Viterbi.png">
<meta property="og:image" content="http://sunyinge.github.io/images/PGM.png">
<meta property="article:published_time" content="2020-06-24T04:12:45.000Z">
<meta property="article:modified_time" content="2020-07-01T03:28:20.470Z">
<meta property="article:author" content="Yinge Sun">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://sunyinge.github.io/images/Viterbi.png">
  
    <link rel="alternative" href="/atom.xml" title="Study notes" type="application/atom+xml">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        
          <a id="nav-github" class="nav-icon" href="https://github.com/sunyinge" target="_blank" rel="noopener"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Study notes</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Hello!</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
      <a class="main-nav-link st-search-show-outputs">Search</a>
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-Probabilistic-graphic-models-for-sequence-tagging" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2020/06/23/Probabilistic-graphic-models-for-sequence-tagging/" class="article-date">
  <time datetime="2020-06-24T04:12:45.000Z" itemprop="datePublished">2020-06-23</time>
</h3>
    
  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Probabilistic graphic models for sequence tagging
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>Sequence tagging, such as <strong>part of speech (POS) tagging</strong> and <strong>named entity recognition (NER)</strong>.</p>
<p>Let <span class="math inline">\(\mathbf{x}=x_1,x_2,\ldots,x_T\)</span> be a sequence of words and <span class="math inline">\(\mathbf{y}=y_1,y_2,\ldots,y_T\)</span> be a sequence of their tags.</p>
<h3 id="generative-models">1. Generative Models</h3>
<p>A Generative Model ‌learns the <strong>joint probability distribution p(x,y).</strong> It predicts the conditional probability with the help of Bayes Theorem.</p>
<h4 id="hidden-markov-models-hmm">1. Hidden Markov Models (HMM)</h4>
<h5 id="the-model">1. The model</h5>
<p><span class="math display">\[
\mathbf{y}=\underset{y}{\text{argmax}}\  p(\mathbf{y}|\mathbf{x})=\underset{y}{\text{argmax}}\ p(\mathbf{x},\mathbf{y})
\]</span></p>
<ul>
<li><p>Markov assumption: <span class="math inline">\(p(\mathbf{y})=\prod_{t=1}^Tp(y_t|y_{t-1})\)</span></p></li>
<li><p>Independence assumption: <span class="math inline">\(p(\mathbf{x}|\mathbf{y})=\prod_{t=1}^Tp(x_t|y_{t})\)</span></p></li>
</ul>
<p>Then we have, <span class="math display">\[
p(\mathbf{x},\mathbf{y})=p(\mathbf{x}|\mathbf{y})p(\mathbf{y})=\prod_{t=1}^Tp(x_t|y_{t})p(y_t|y_{t-1}).
\]</span> A HMM is specified by</p>
<a id="more"></a>
<ul>
<li><p>Hidden states <span class="math inline">\(\mathbf{S}=s_1,\ldots,s_N\)</span></p></li>
<li>Start states <span class="math inline">\(s_0\)</span></li>
<li><strong>Transition probabilities</strong> <span class="math inline">\(A_{(N+1)\times N}\)</span> with <span class="math inline">\(a_{ij}=p(s_j|s_i)\)</span> (row sum equals to 1)</li>
<li>Possible visible outcomes <span class="math inline">\(\mathbf{O}\)</span></li>
<li><p><strong>Output probabilities</strong> <span class="math inline">\(B_{N\times K}\)</span> with <span class="math inline">\(b_{ik}=p(o_k|s_i)\)</span> (row sum equals to 1)</p></li>
</ul>
<p>So there are <span class="math inline">\((N+1)N+NK\)</span> parameters in HMM.</p>
<h5 id="training-in-hmm">2. Training in HMM</h5>
<ul>
<li>supervised case: MLE</li>
</ul>
<p><span class="math display">\[
a_{ij}=p(s_j|s_i)=\frac{\sum_{t=1}^T[y_{t-1}=s_i,y_t=s_j]}{\sum_{t=1}^T[y_t=s_i]}
\]</span></p>
<p>​ where <span class="math inline">\([]\)</span> is the indicator and <span class="math inline">\(\prod_{t=1}^T[y_{t-1}=s_i,y_t=s_j]\)</span> is count(<span class="math inline">\(s_i\rightarrow s_j\)</span>).</p>
<ul>
<li><p>unsupervised case <span class="math display">\[
a_{ij}=p(s_j|s_i)=\frac{\sum_{t=1}^Tp(y_{t-1}=s_i,y_t=s_j)}{\sum_{t=1}^Tp(y_t=s_i)}
\]</span> EM algorithm: E step to get posterior probabilities for sqequence of hidden variables <span class="math inline">\(p(\mathbf{y}|\mathbf{x})\)</span> by fixing the parameters (initialization at the first step) and M step to update parameters.</p>
<p><strong>Baum-Welch algorithm</strong>:</p>
<ul>
<li><p>E step: posterior probabilities for 2 sequential hidden variables <span class="math display">\[
p(y_{t-1}=s_i,y_t=s_j)
\]</span> Can be effectively done with dynamic programming (forward-backward algorithm)</p></li>
<li><p>M step: maximum likelihood updates for the parameters <span class="math display">\[
a_{ij}=p(s_j|s_i)=\frac{\sum_{t=1}^Tp(y_{t-1}=s_i,y_t=s_j)}{\sum_{t=1}^Tp(y_t=s_i)}
\]</span></p></li>
</ul></li>
</ul>
<h5 id="decoding-in-hmm-prediction">3. Decoding in HMM (prediction)</h5>
<p>Dynamic programming: <span class="math inline">\(O(N^2T)\)</span></p>
<p><strong>Viterbi algorithm</strong>:</p>
<ul>
<li><span class="math inline">\(Q_{t,s}\)</span>: most probable sequence of hidden states that finishes with the state <span class="math inline">\(s\)</span> and generate <span class="math inline">\(o_1,o_2,\ldots,o_t\)</span>.</li>
<li><span class="math inline">\(q_{t,s}\)</span>: the probability of the sequence <span class="math inline">\(Q_{t,s}\)</span>.</li>
</ul>
<p>Then <span class="math display">\[
q_{t,s}=\underset{s&#39;}{max}\ q_{t-1,s&#39;}\cdot p(s|s&#39;)\cdot p(o_t|s)
\]</span> <img src="/images/Viterbi.png" /></p>
<h3 id="discriminative-models">2. Discriminative Models</h3>
<p>A Discriminative Model ‌learns the <strong>conditional probability distribution p(y|x)</strong>. It predicts the conditional probability.</p>
<h4 id="maximum-entropy-markov-model-memm">1. Maximum Entropy Markov Model (MEMM)</h4>
<p><span class="math display">\[
p(\mathbf{y}|\mathbf{x})=\prod_{t=1}^Tp(y_t|y_{t-1},x_t),\\
\text{where }p(y_t|y_{t-1},x_t)=\frac{1}{Z_t(y_{t-1},x_t)}\cdot \text{exp}(\sum_{k=1}^K\theta_k f_k(y_t,y_{t-1},x_t)),\\
\text{where }Z_t(y_{t-1},x_t) \text{ is the normalization constant, } \theta_k \text{ is the weight and } f_k(y_t,y_{t-1},x_t)) \text{ is the feature.}
\]</span></p>
<h4 id="conditional-random-field-crf">2. Conditional Random Field (CRF)</h4>
<ul>
<li><p>Linear chain: <span class="math display">\[
p(\mathbf{y}|\mathbf{x})=\frac{1}{Z(\mathbf{x})}\prod_{t=1}^T\text{exp}(\sum_{k=1}^K\theta_k f_k(y_t,y_{t-1},x_t))
\]</span></p></li>
<li><p>General form: <span class="math display">\[
p(\mathbf{y}|\mathbf{x})=\frac{1}{Z(\mathbf{x})}\prod_{a=1}^a\Psi_a(y_a,x_a)
\]</span></p></li>
</ul>
<p><img src="/images/PGM.png" /></p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li></ul>

        <a data-url="http://sunyinge.github.io/2020/06/23/Probabilistic-graphic-models-for-sequence-tagging/" data-id="ckcn0j4zz000g5j3x30u3fw7d" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/06/24/Python/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Python
        
      </div>
    </a>
  
  
    <a href="/2020/06/09/End-symbol-in-n-gram-models/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">What&#39;s the real need for an end-symbol in n-gram models?</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/07/14/Maximum-flow/">Maximum flow</a>
          </li>
        
          <li>
            <a href="/2020/07/14/Leetcode-error-prone/">Leetcode error-prone</a>
          </li>
        
          <li>
            <a href="/2020/07/13/Transfer-learning/">Transfer learning</a>
          </li>
        
          <li>
            <a href="/2020/07/12/All-pairs-shortest-paths/">All-pairs shortest paths</a>
          </li>
        
          <li>
            <a href="/2020/07/11/Single-source-shortest-path/">Single-source shortest path</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/" rel="tag">Algorithm</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DL/" rel="tag">DL</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Installation/" rel="tag">Installation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Leetcode/" rel="tag">Leetcode</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/" rel="tag">ML</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Optimization/" rel="tag">Optimization</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">10</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Yinge Sun<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
  <a href="#search" class="mobile-nav-link st-search-show-outputs">Search</a>
</nav>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    menuSettings: {
      zoom: "None"
    },
    showMathMenu: false,
    jax: ["input/TeX","output/CommonHTML"],
    extensions: ["tex2jax.js"],
    TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js"],
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
  });
</script>

<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.1/MathJax.js"></script>



<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


</div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
